## Data Quality for ML Based Software Security
Dr. Ali Babar - CREST

#lookup 
digital twins
CPS in context of IOT

###### Motivation
- Aware of software libraries (log4j)
- Software Bill of Material (SBOM) is ineffective in answering critical questions
	- do we really know what software is in our organization
	- how do we establish trust in security of such software

#### CREST Data Centric Software Security Research
- No perfectly clean dataset of vulnerabilities
	- Label noise
		- false positives, constant new vulnerabilities
	- Data noise
		- duplicates
- => Analyze data => noise-tolerant learning techniques
#### Data-Centric Software QA
Data requirements determine the types and source of data for building a model
- Data Wrangling => collection, labeling, cleaning
	- Can take up to 25% of project time
Software Vulnerability Prevention
- ML dependent
	- Consideration
		- data requirements vary based on language, capabilities of model
			- Application context, SV type, language, granularity
		- data come from real-world, synthetic, mixed code
			- trade-off between scarcity & realism
		- Gathered data need labelling, provided by developers (NVD)
		- Labelling non-vulnerable class is problematic
#### Data Quality Problems Experienced/Observed
##### Scarcity
- 100,000's of vulns & million of images (CV)
- Security issues <10% of all reported issues
* <= Lack of labelling, imperfect data collection, rare sec. issues
* => imbalanced data, lack of data, _more (high quality) data beats cleverer algorithm_
##### Data Inaccuracy
* Non-sec issues labeled as such
* vulns unfixed
* wasted inspection effort
* <= unknown unknowns, lack of reporting, tangled issues
* => systematic labeling errors > random errors, introducing back doors
##### Data Irrelevance
- Not all input is useful for SVP
- Code comments for predicting vulns?
- Detecting versioning?
- <= lack of domain expertise (NLPers & SSE??), lack of data exploratory analysis
- => Negatively affecting construct validity, reducing model performance (comments), reduced SVP performance
##### Data Redundancy
- Forks, clones, but vulns are only fixed at source => same vulns found across different software versions/branches/projects
- <= cloned projects, merged code from features > master, renamed files/funcs with same code, cosmetic changes (formating)
- => Limits learning capabilities, etc
##### Data Mis-coverage
- SVPs could span multiple lines, files, etc, but model only looks at narrow scope
- <= Finer granularity > larger models, truncated input required for some (DL) models
- => Missing context
##### Data Non-representativeness
- Synthetic may not properly represent real vuln issues
- <= synthetic data, diff feature between apps
##### Data Drift
- Shifting threat landscape
- <= new terms, implementation
- => degraded performance, non-temporal evaluation technique
##### Data Inaccessibility
- security data not always shared
- shared incomplete data may differ from original data
- <= privacy, data change, data size
- => limited reproducibility
##### Data Reuse
- needle = SVP in haystack = data
- prefer to reuse data than update/collect data
- <= unsuitable infrastructure, trusting on datasets
##### Data Maliciousness
- Using/sharing data w/o precautions
- Not reporting 
#### Recommendations
- ID missing vuln data
- Mitigate label noise
- Update data sets
- Dataviz => acquire help of non data scientists
- Use diverse language sets
- Use of data quality assessment criteria
- Data sharing + governance



https://www.mayhem.security/ - fuzzing tool